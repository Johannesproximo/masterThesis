{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ed6b3e7",
   "metadata": {},
   "source": [
    "# Notebook overview\n",
    "Downloads or copies images referenced in CSV metadata files for the binary GBIF dataset, using a local cache when possible and concurrent downloads for missing images.\n",
    "\n",
    "- Selects relevant CSVs files in a folder and processes each file\n",
    "- Copies available images from a cache; downloads missing images with ThreadPoolExecutor\n",
    "- Adds tracking columns ('image_downloaded', 'image_download_fail_reason') and saves updated CSVs\n",
    "- Handles hostname-specific SSL verification\n",
    "\n",
    "The notebook was exported as a Python script and run in a console using Tmux to execute it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100157b58471c722",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-25T15:47:35.634100Z",
     "start_time": "2025-01-25T15:47:34.604606Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "from typing import Optional\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2e6fdb9efe7dc1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-25T15:47:37.753809Z",
     "start_time": "2025-01-25T15:47:37.750841Z"
    }
   },
   "outputs": [],
   "source": [
    "# Number of Threads\n",
    "MAX_WORKERS = 12\n",
    "\n",
    "# Folder with CSV files (selects all relevant csv files in the folder - function select_files)\n",
    "CSV_FOLDER_PATH = r'/home/jleick/masterArbeitProjekt/data/ami_dataset/ami_gbif/fine-grained_classification/metadata'\n",
    "\n",
    "# Folder to save the downloaded pictures (creates a folder for each csv file)\n",
    "DESTINATION_PATH = '/home/jleick/masterArbeitProjekt/data/ami_download/ami_gbif/fetched_images_binary'\n",
    "\n",
    "# Cache folder for available pictures\n",
    "CACHE = r'/home/jleick/masterArbeitProjekt/data/ami_download/ami_gbif/cached_images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd2bf13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04_ami-gbif_fine-grained_all_test.csv\n",
      "04_ami-gbif_fine-grained_all_val.csv\n",
      "04_ami-gbif_fine-grained_all_train.csv\n"
     ]
    }
   ],
   "source": [
    "### Select csv files in given folder Path\n",
    "\n",
    "def select_files(csv_folder_path: str):\n",
    "    folder_contains = os.listdir(csv_folder_path)\n",
    "    folder_filtered = []\n",
    "    # Filter all relevant files\n",
    "    for filename in folder_contains:\n",
    "        if ('.csv' in filename and 'all' in filename):\n",
    "            folder_filtered.append(filename)\n",
    "\n",
    "    # Remove the .csv files from the selection for which a 'download.csv' file already exists.\n",
    "    for filename in folder_contains:\n",
    "        if ('.csv' in filename and 'all' in filename and 'download' in filename):\n",
    "            filename = filename.replace('_download.csv', '.csv')\n",
    "            if filename in folder_filtered:\n",
    "                folder_filtered.remove(filename)\n",
    "    \n",
    "    return folder_filtered\n",
    "\n",
    "#call funktion\n",
    "folder_filtered = select_files(CSV_FOLDER_PATH)\n",
    "\n",
    "# Print all selected files\n",
    "for folder in folder_filtered:\n",
    "    print(folder)\n",
    "\n",
    "# DUBLICATED CODE (CODE EXIST IN OTHER FILE TOO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406edbe62b6b5fac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-25T15:47:48.879902Z",
     "start_time": "2025-01-25T15:47:47.660048Z"
    }
   },
   "outputs": [],
   "source": [
    "### load .csv file into pandas DataFrame (optional with number of rows for testing)\n",
    "\n",
    "def load_data(data_path: str, nrows: Optional[int] = None):\n",
    "    # Load data into Pandas DataFrame\n",
    "    df = pd.read_csv(data_path, sep=',', header='infer', nrows=nrows)\n",
    "    # print(f'>>> {df.shape} - Shape of: {data_path.split(\"/\")[-1]}')\n",
    "\n",
    "    # Print Columns with only NaN in Column\n",
    "    # empty_columns = df.columns[df.isna().all()]\n",
    "    # print(f'>>> column: {list(empty_columns)} contains only NaN')\n",
    "\n",
    "    # Check if df contains duplicated URLs\n",
    "    if not df['identifier'].is_unique:\n",
    "        duplicates = df['identifier'].duplicated(keep=False).sum()\n",
    "        print(f'Duplicate: {duplicates} are included in identifier')\n",
    "        #raise Exception('Duplicates are included in identifier') # assert url_list.is_unique\n",
    "\n",
    "    return df\n",
    "\n",
    "### Test function\n",
    "# test_path = os.path.join(CSV_FOLDER_PATH, folder_filtered[0])\n",
    "# data = load_data(test_path, 5)\n",
    "# data\n",
    "\n",
    "# DUBLICATED CODE (CODE EXIST IN OTHER FILE TOO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b711951",
   "metadata": {},
   "outputs": [],
   "source": [
    "### add column 'image_downloaded' and 'image_download_fail_reason' to df to track download\n",
    "\n",
    "def add_download_Columns_to_df(df: pd.DataFrame):\n",
    "\n",
    "    column_name_download = 'image_downloaded'\n",
    "    if column_name_download not in df.columns:\n",
    "        df[column_name_download] = False\n",
    "        print(f'>>> {df.shape} - Added column: {column_name_download}')\n",
    "\n",
    "    column_name_download_fail = 'image_download_fail_reason'\n",
    "    if column_name_download_fail not in df.columns:\n",
    "        df[column_name_download_fail] = False\n",
    "        print(f'>>> {df.shape} - Added column: {column_name_download_fail}')\n",
    "    \n",
    "    return column_name_download, column_name_download_fail\n",
    "\n",
    "### Test function\n",
    "# column_name_download, column_name_download_fail = add_download_Columns_to_df(data)\n",
    "# print( f'return: {column_name_download}' )\n",
    "# print( f'return: {column_name_download_fail}' )\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c548ea1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### copy availables images from cache to destination\n",
    "\n",
    "def copy_availabe_images(df: pd.DataFrame, source_dir: str, destination_dir: str, column_name_download: str, column_name_download_fail: str):\n",
    "    for index, image_pfad in df['image_path'].items():\n",
    "        source_path = os.path.join(source_dir, image_pfad)\n",
    "\n",
    "        if os.path.exists(source_path):\n",
    "            try:\n",
    "                destination_path = os.path.join(destination_dir, image_pfad)\n",
    "                if not os.path.exists(destination_path):\n",
    "                    os.makedirs(os.path.dirname(destination_path), exist_ok=True)\n",
    "                    shutil.copyfile(source_path, destination_path)\n",
    "\n",
    "                    df.at[index, column_name_download] = True\n",
    "                    df.at[index, column_name_download_fail] = 'no error'\n",
    "                    print(f'Image (at index: {index}) found and copied: {image_pfad}')\n",
    "                else:\n",
    "                    df.at[index, column_name_download] = True\n",
    "                    df.at[index, column_name_download_fail] = 'no error'\n",
    "                    print(f'Image (at index: {index}) exist already - no copy necessary : {image_pfad}')\n",
    "            except Exception as e:\n",
    "                print(f'Error Image (at index: {index}) copying image {image_pfad}: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f6640c5377c6d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-25T15:48:38.818010Z",
     "start_time": "2025-01-25T15:48:38.809115Z"
    }
   },
   "outputs": [],
   "source": [
    "### download missing images\n",
    "\n",
    "def download_image(index: int, df: pd.DataFrame, destination_dir, column_name_download: str, column_name_download_fail: str):\n",
    "    if not df.at[index, column_name_download]:\n",
    "        url = df.at[index, 'identifier']\n",
    "        image_path = df.at[index, 'image_path']\n",
    "        destination_path = os.path.join(destination_dir, image_path)\n",
    "        os.makedirs(os.path.dirname(destination_path), exist_ok=True)\n",
    "\n",
    "        ### Start - Addition to deactivate verification for specific hostnames\n",
    "        problematic_host = 'monarch.calacademy.org' # Addition to deactivate verification for specific hostnames\n",
    "\n",
    "        try:\n",
    "            # Analyze the URL to get the hostname\n",
    "            parsed_url = urlparse(url)\n",
    "            hostname = parsed_url.netloc # for example 'monarch.calacademy.org'\n",
    "\n",
    "            # Set the verify parameter based on the hostname\n",
    "            should_verify = True # Always verify by default\n",
    "            if hostname == problematic_host:\n",
    "                should_verify = False # Deactivate only for the problematic domain\n",
    "                print(f' Deaktiviere SSL-Verifizierung fÃ¼r {url}')\n",
    "            \n",
    "            with requests.get(url, stream=True, timeout=300, verify=should_verify) as response:\n",
    "            ### Ende - Addition to deactivate verification for specific hostnames\n",
    "\n",
    "                response.raise_for_status()\n",
    "#               with open(destination_path, 'wb') as file:\n",
    "#                   shutil.copyfileobj(response.raw, file)\n",
    "                with open(destination_path, 'wb') as file:\n",
    "                    for chunk in response.iter_content(chunk_size=8192):\n",
    "                        file.write(chunk)\n",
    "\n",
    "            df.at[index, 'image_downloaded'] = True\n",
    "            df.at[index, 'image_download_fail_reason'] = 'no error'\n",
    "            print(f'{index}: Successful download {url}')\n",
    "        except Exception as e:\n",
    "            df.at[index, 'image_downloaded'] = False\n",
    "            df.at[index, 'image_download_fail_reason'] = f'{e}'\n",
    "            print(f'{index}: Failed download {url}: {e}')\n",
    "        return image_path\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "58781f1b89d4ba23",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-25T15:52:29.053156Z",
     "start_time": "2025-01-25T15:48:45.137379Z"
    }
   },
   "outputs": [],
   "source": [
    "### Execute download_image function with ThreadPoolExecutor\n",
    "\n",
    "def download_images_with_executorpool(max_workers: int, download_image: callable, df: pd.DataFrame, destination: str, column_name_download: str, column_name_download_fail: str):\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "         futures = [executor.submit(download_image, index, df, destination, column_name_download, column_name_download_fail) for index in df.index]\n",
    "         for future in as_completed(futures):\n",
    "            try:\n",
    "                future.result()\n",
    "            except Exception as e:\n",
    "                print(f'Error while retreving result from future: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b7020a8b9f5c28f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-25T15:57:42.931916Z",
     "start_time": "2025-01-25T15:57:39.522133Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------Variables----------------\n",
      ">>> Processing CSV file from: /home/jleick/masterArbeitProjekt/data/ami_dataset/ami_gbif/fine-grained_classification/metadata/04_ami-gbif_fine-grained_all_test.csv\n",
      ">>> Saving CSV file to: /home/jleick/masterArbeitProjekt/data/ami_dataset/ami_gbif/fine-grained_classification/metadata/04_ami-gbif_fine-grained_all_test_download.csv\n",
      ">>> Destination folder: /home/jleick/masterArbeitProjekt/data/ami_download_temp/ami_gbif/fetched_images_fine-grained/04_ami-gbif_fine-grained_all_test/\n",
      ">>> Cache folder: /home/jleick/masterArbeitProjekt/data/ami_download/ami_gbif/cached_images\n",
      ">>> Load data and add column to track existing downloads\n",
      ">>> (5, 17) - Added column: image_downloaded\n",
      ">>> (5, 18) - Added column: image_download_fail_reason_1\n",
      ">>> Starting copy availabe images from cache to destination\n",
      "Image (at index: 0) found and copied: 50c9509d-22c7-4a22-a47d-8c48425ef4a7/1024178685.jpg\n",
      "Image (at index: 1) found and copied: 50c9509d-22c7-4a22-a47d-8c48425ef4a7/1024178685_1.jpg\n",
      "Image (at index: 2) found and copied: 50c9509d-22c7-4a22-a47d-8c48425ef4a7/1024178685_2.jpg\n",
      "Image (at index: 3) found and copied: 50c9509d-22c7-4a22-a47d-8c48425ef4a7/1024183436.jpg\n",
      "Image (at index: 4) found and copied: 50c9509d-22c7-4a22-a47d-8c48425ef4a7/1024184322.jpg\n",
      ">>> Starting downloading images with 12 workers\n",
      ">>> Save updated Dataframe to: /home/jleick/masterArbeitProjekt/data/ami_dataset/ami_gbif/fine-grained_classification/metadata/04_ami-gbif_fine-grained_all_test_download.csv\n",
      ">>> FINISH Process: 04_ami-gbif_fine-grained_all_test.csv\n",
      "\n",
      "\n",
      "-------------Variables----------------\n",
      ">>> Processing CSV file from: /home/jleick/masterArbeitProjekt/data/ami_dataset/ami_gbif/fine-grained_classification/metadata/04_ami-gbif_fine-grained_all_val.csv\n",
      ">>> Saving CSV file to: /home/jleick/masterArbeitProjekt/data/ami_dataset/ami_gbif/fine-grained_classification/metadata/04_ami-gbif_fine-grained_all_val_download.csv\n",
      ">>> Destination folder: /home/jleick/masterArbeitProjekt/data/ami_download_temp/ami_gbif/fetched_images_fine-grained/04_ami-gbif_fine-grained_all_val/\n",
      ">>> Cache folder: /home/jleick/masterArbeitProjekt/data/ami_download/ami_gbif/cached_images\n",
      ">>> Load data and add column to track existing downloads\n",
      ">>> (5, 17) - Added column: image_downloaded\n",
      ">>> (5, 18) - Added column: image_download_fail_reason_1\n",
      ">>> Starting copy availabe images from cache to destination\n",
      "Image (at index: 2) found and copied: 50c9509d-22c7-4a22-a47d-8c48425ef4a7/1024197116.jpg\n",
      "Image (at index: 3) found and copied: 96404cc2-f762-11e1-a439-00145eb45e9a/1039035961.jpg\n",
      ">>> Starting downloading images with 12 workers\n",
      "1: Successful download https://inaturalist-open-data.s3.amazonaws.com/photos/1002566/original.jpg\n",
      "0: Successful download https://inaturalist-open-data.s3.amazonaws.com/photos/453118/original.jpg\n",
      "4: Successful download https://images.collections.yale.edu/iiif/2/ypm:b3099f0a-c73f-46a0-8dcf-e5f788d0752c/full/!1920,1920/0/default.jpg\n",
      ">>> Save updated Dataframe to: /home/jleick/masterArbeitProjekt/data/ami_dataset/ami_gbif/fine-grained_classification/metadata/04_ami-gbif_fine-grained_all_val_download.csv\n",
      ">>> FINISH Process: 04_ami-gbif_fine-grained_all_val.csv\n",
      "\n",
      "\n",
      "-------------Variables----------------\n",
      ">>> Processing CSV file from: /home/jleick/masterArbeitProjekt/data/ami_dataset/ami_gbif/fine-grained_classification/metadata/04_ami-gbif_fine-grained_all_train.csv\n",
      ">>> Saving CSV file to: /home/jleick/masterArbeitProjekt/data/ami_dataset/ami_gbif/fine-grained_classification/metadata/04_ami-gbif_fine-grained_all_train_download.csv\n",
      ">>> Destination folder: /home/jleick/masterArbeitProjekt/data/ami_download_temp/ami_gbif/fetched_images_fine-grained/04_ami-gbif_fine-grained_all_train/\n",
      ">>> Cache folder: /home/jleick/masterArbeitProjekt/data/ami_download/ami_gbif/cached_images\n",
      ">>> Load data and add column to track existing downloads\n",
      ">>> (5, 17) - Added column: image_downloaded\n",
      ">>> (5, 18) - Added column: image_download_fail_reason_1\n",
      ">>> Starting copy availabe images from cache to destination\n",
      "Image (at index: 4) found and copied: 50c9509d-22c7-4a22-a47d-8c48425ef4a7/1024184183.jpg\n",
      ">>> Starting downloading images with 12 workers\n",
      "0: Successful download https://inaturalist-open-data.s3.amazonaws.com/photos/163324/original.jpg\n",
      "2: Successful download https://inaturalist-open-data.s3.amazonaws.com/photos/954969/original.JPG\n",
      "3: Successful download https://inaturalist-open-data.s3.amazonaws.com/photos/958208/original.JPG\n",
      "1: Successful download https://inaturalist-open-data.s3.amazonaws.com/photos/940021/original.jpg\n",
      ">>> Save updated Dataframe to: /home/jleick/masterArbeitProjekt/data/ami_dataset/ami_gbif/fine-grained_classification/metadata/04_ami-gbif_fine-grained_all_train_download.csv\n",
      ">>> FINISH Process: 04_ami-gbif_fine-grained_all_train.csv\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### RUN - call functions to process file\n",
    "\n",
    "def process_file(filename: str, csv_folder_path: str, destination_path: str, CACHE: str, MAX_WORKERS: int):\n",
    "    # create file paths from filename\n",
    "    DATA = os.path.join(csv_folder_path, filename)\n",
    "    if ('download' not in filename):\n",
    "        DATA_UPDATE = DATA.replace('.csv', '_download.csv')\n",
    "    else:\n",
    "        DATA_UPDATE = DATA\n",
    "\n",
    "    if ('download' in filename):\n",
    "        filename = filename.replace('_download.csv', '.csv')\n",
    "    DESTINATION = os.path.join(destination_path, filename.replace('.csv', '/'))\n",
    "    print('-------------Variables----------------')\n",
    "    print(f'>>> Processing CSV file from: {DATA}')\n",
    "    print(f'>>> Saving CSV file to: {DATA_UPDATE}')\n",
    "    print(f'>>> Destination folder: {DESTINATION}')\n",
    "    print(f'>>> Cache folder: {CACHE}')\n",
    "\n",
    "    # process file\n",
    "    print(f'>>> Load data and add column to track existing downloads')\n",
    "    df = load_data(DATA, 5)\n",
    "    column_name_download, column_name_download_fail = add_download_Columns_to_df(df)\n",
    "\n",
    "    print(f'>>> Starting copy availabe images from cache to destination')\n",
    "    copy_availabe_images(df, CACHE, DESTINATION, column_name_download, column_name_download_fail)\n",
    "\n",
    "    print(f'>>> Starting downloading images with {MAX_WORKERS} workers')\n",
    "    download_images_with_executorpool(MAX_WORKERS, download_image, df, DESTINATION, column_name_download, column_name_download_fail)\n",
    "\n",
    "    print(f'>>> Save updated Dataframe to: {DATA_UPDATE}')\n",
    "    df.to_csv(DATA_UPDATE, na_rep=\"NULL\", index=False)\n",
    "\n",
    "    #print('>>> Copy files to cache if not available')\n",
    "    #subprocess.run(f'rsync -av --ignore-existing {DESTINATION} {CACHE}', shell=True, check=True)\n",
    "    # !rsync -av --ignore-existing {DESTINATION} {CACHE}\n",
    "\n",
    "    print(f'>>> FINISH Process: {filename}\\n\\n')\n",
    "\n",
    "\n",
    "### call funktions\n",
    "for filename in folder_filtered:\n",
    "    process_file(filename, CSV_FOLDER_PATH, DESTINATION_PATH, CACHE, MAX_WORKERS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masterArbeit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
