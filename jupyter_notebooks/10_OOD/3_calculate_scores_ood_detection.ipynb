{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "009530ce",
   "metadata": {},
   "source": [
    "# Notebook overview\n",
    "\n",
    "Evaluates the performance of DKNN (OOD) classifier by calculating metrics for different threshold(classifier) variants.\n",
    "\n",
    "- Loads prediction results containing OOD detection flags.\n",
    "- Calculates classification metrics: TPR, FPR, AUROC and FPR95.\n",
    "- Saves the calculated metrics to CSV files.\n",
    "\n",
    "The notebook was used for both datasets (“original” and “resized”); only the path variables need to be adapted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1836355",
   "metadata": {},
   "source": [
    "# Preperation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d726df4a",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3cd6f30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "import ast\n",
    "\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb50623",
   "metadata": {},
   "source": [
    "### Path - prediction_dir_path, result_dir_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f32931d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### The paths for the original and resized data must be adjusted for calculation of original or resized ood Scores (Replace “origin” with “resized” and vice versa).\n",
    "use_dataset = 'resized'\n",
    "# use_dataset = 'origin'\n",
    "\n",
    "### prediction folder to load df\n",
    "PREDICTION_DIR_PATH = rf'/home/stud/jleick/masterArbeitProjekt/models/ood/predictions/{use_dataset}'\n",
    "prediction_dir_path = Path(PREDICTION_DIR_PATH)\n",
    "if not prediction_dir_path.exists():\n",
    "    raise FileNotFoundError(f\"Folder does not exist: {PREDICTION_DIR_PATH}\")\n",
    "\n",
    "### Folder to save results\n",
    "RESULT_DIR_PATH = rf'//home/stud/jleick/masterArbeitProjekt/models/ood/scores_ood_detection_(corrected)/{use_dataset}'\n",
    "\n",
    "result_dir_path = Path(RESULT_DIR_PATH)\n",
    "if not result_dir_path.exists():\n",
    "    raise FileNotFoundError(f\"Folder does not exist: {RESULT_DIR_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73a0acd",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a30e768",
   "metadata": {},
   "source": [
    "### Function - predict_id_scores_over_percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8f6384a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcuation TN and FP for testsets which are containing only ID sampels (TN and FP fields of a binary confusion matrix)\n",
    "# OOD Classifier classifies OOD Sampel as True\n",
    "\n",
    "# For the following calculation, it helps to imagine that the ID data set represents only half of the two-class confusion matrix.\n",
    "# An ID data set describes only the TN and FP fields, while an OOD data set describes only the TP and FN fields.\n",
    "\n",
    "# Calculates TNR of samples based on data which contains only ID Samples\n",
    "# Calculates FNR of samples based on data which contains only OOD Samples\n",
    "\n",
    "def predict_id_scores_over_percentile(prediction_df:pd.DataFrame, percentiles: pd.Series) -> dict[float,float]:\n",
    "    id_scores = {}\n",
    "    id_scores['0'] = 0 # Add 0 - for visualisation and AUROC calculation - Calculation with Threshold 0 was not calculated\n",
    "\n",
    "    for percentile in percentiles:\n",
    "        is_ood = (prediction_df[f'ood_{percentile}'] == -1).value_counts()\n",
    "\n",
    "        is_ood = is_ood.reindex([True, False], fill_value=0) \n",
    "        # If there is no 'True' or 'False' value, fill empty values with 0. For example, when there are no 'False' values, reindex creates 'False' = 0. \n",
    "\n",
    "        id_score = is_ood[False] / (is_ood[True] + is_ood[False])        \n",
    "        # is_ood[False] -> means for sampels (Datasets) wich are only ID they are TN\n",
    "        # is_ood[True] -> means for sampels (Datasets) which are only ID they are FP\n",
    "\n",
    "        # is_ood[False] -> means for sampels (Datasets) wich are only OOD they are FN\n",
    "        # is_ood[True] -> means for sampels (Datasets) which are only OOD they are TP\n",
    "\n",
    "        id_scores[percentile] = id_score\n",
    "        # print(f'percentile: {percentile} - {id_score} ')\n",
    "\n",
    "    id_scores['1'] = 1 # Add 0 - for visualisation and AUROC calculation - Calculation with Threshold max was not calculated\n",
    "    \n",
    "    return id_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bff52e8",
   "metadata": {},
   "source": [
    "### Function - predict_ood_scores_over_percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "18463410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcuation TP and FN for testsets which are containing only OOD sampels (TP and FN fields of a binary confusion matrix)\n",
    "# OOD Classifier classifies OOD Sampel as True\n",
    "\n",
    "# For the following calculation, it helps to imagine that the OOD data set represents only half of the two-class confusion matrix.\n",
    "# An OOD data set describes only the TP and FN fields, while an ID data set describes only the TN and FP fields.\n",
    "\n",
    "# Calculates TPR of samples based on data which contains only OOD Samples\n",
    "# Calculates FPR of samples based on data which contains only ID Samples\n",
    "\n",
    "def predict_ood_scores_over_percentile(prediction_df:pd.DataFrame, percentiles: pd.Series) -> dict[float,float]:\n",
    "    ood_scores = {}\n",
    "    ood_scores[0] = 1 # Add 0 - for visualisation - Calculation with Threshold 0 was not calculated\n",
    "\n",
    "    for percentile in percentiles:\n",
    "        is_ood = (prediction_df[f'ood_{percentile}'] == -1).value_counts()\n",
    "\n",
    "        is_ood = is_ood.reindex([True, False], fill_value=0)\n",
    "        # If there is no 'True' or 'False' value, fill empty values with 0. For example, when there are no 'False' values, reindex creates 'False' = 0. \n",
    "\n",
    "        ood_score = is_ood[True] / (is_ood[True] + is_ood[False])\n",
    "        # is_ood[True] -> means for sampels (Datasets) which are only OOD they are TP\n",
    "        # is_ood[True] -> means for sampels (Datasets) which are only ID they are FP\n",
    "\n",
    "        ood_scores[percentile] = ood_score\n",
    "        # print(f'percentile: {percentile} - {ood_score} ')\n",
    "\n",
    "    ood_scores[1] = 0 # Add 0 - for visualisation - Calculation with Threshold max was not calculated\n",
    "    return ood_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120a6b00",
   "metadata": {},
   "source": [
    "### Function - calculate_fpr95 - get_closest_fpr95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fba08dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_fpr95( tpr: pd.Series, fpr: pd.Series) -> float:\n",
    "    tpr_rev = tpr.values[::-1]\n",
    "    fpr_rev = fpr.values[::-1]\n",
    "    fpr_at_95_tpr = np.interp(0.95, tpr_rev, fpr_rev)\n",
    "    return fpr_at_95_tpr\n",
    "\n",
    "\n",
    "def get_closest_fpr95( tpr: pd.Series, fpr: pd.Series) -> tuple[float,float]:\n",
    "    idx_95 = np.argmin(np.abs(tpr - 0.95))\n",
    "    tpr_at_95 = tpr[idx_95]\n",
    "    fpr_at_95_tpr = fpr[idx_95]\n",
    "    return tpr_at_95, fpr_at_95_tpr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aeb3239",
   "metadata": {},
   "source": [
    "### Function - run_predict_scores_over_percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c39a2fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_predict_scores_over_percentile( prediction_load_path: Path, score_save_path: Path, percentiles:pd.Series):\n",
    "    ### high\n",
    "    # load predictions high\n",
    "    high_id_test_prediction_df = pd.read_csv( prediction_load_path / 'high_id_test_prediction_ood.csv', index_col=False, converters={\"k_distances\": ast.literal_eval})\n",
    "    high_ood_test_prediction_df = pd.read_csv( prediction_load_path / 'high_ood_test_prediction_ood.csv', index_col=False, converters={\"k_distances\": ast.literal_eval})\n",
    "\n",
    "    # calculate scores high\n",
    "    high_TPR_scores_dict = predict_ood_scores_over_percentile( high_ood_test_prediction_df, percentiles ) # TPR\n",
    "    high_FPR_scores_dict = predict_ood_scores_over_percentile( high_id_test_prediction_df, percentiles ) # TPR\n",
    "\n",
    "    # save scores high\n",
    "    high_df = pd.DataFrame( [high_TPR_scores_dict, high_FPR_scores_dict], index=['TPR_scores', 'FPR_scores'] ).T # TNR, TPR, TPR\n",
    "    high_df.to_csv( score_save_path / 'high_id_ood_scores.csv', index=True)\n",
    "\n",
    "    # calculate AURCO\n",
    "    high_score_auroc = metrics.auc(list(high_FPR_scores_dict.values()), list(high_TPR_scores_dict.values()))\n",
    "\n",
    "    # save scores AURCO\n",
    "    with open(score_save_path / 'high_auroc_score.txt', 'w') as f:\n",
    "        f.write(str(high_score_auroc))\n",
    "\n",
    "    # Calculate FPR95\n",
    "    high_tpr_list = pd.Series(high_TPR_scores_dict.values())\n",
    "    high_fpr_list = pd.Series(high_FPR_scores_dict.values())\n",
    "\n",
    "    high_fpr95_interpolate = calculate_fpr95(high_tpr_list, high_fpr_list)\n",
    "    high_tpr_closest, high_fpr95_closest = get_closest_fpr95(high_tpr_list, high_fpr_list)\n",
    "\n",
    "    # Save FPR95 scores\n",
    "    with open(score_save_path / 'high_fpr95_score.txt', 'w') as f:\n",
    "        f.write(f'FPR95 (interpolated): {high_fpr95_interpolate:.4f}\\n')\n",
    "        f.write(f'TPR closest to 0.95: {high_tpr_closest:.4f}\\n')\n",
    "        f.write(f'FPR95 (closest): {high_fpr95_closest:.4f}\\n')\n",
    "\n",
    "    ### low\n",
    "    # load prediction low    \n",
    "    low_id_test_prediction_df = pd.read_csv( prediction_load_path / 'low_id_test_prediction_ood.csv', index_col=False, converters={\"k_distances\": ast.literal_eval})\n",
    "    low_ood_test_prediction_df = pd.read_csv( prediction_load_path / 'low_ood_test_prediction_ood.csv', index_col=False, converters={\"k_distances\": ast.literal_eval})\n",
    "\n",
    "    # calculate scores low\n",
    "    low_TPR_scores_dict = predict_ood_scores_over_percentile( low_ood_test_prediction_df, percentiles )\n",
    "    low_FPR_scores_dict = predict_ood_scores_over_percentile( low_id_test_prediction_df, percentiles )\n",
    "\n",
    "    # save scores low\n",
    "    low_df = pd.DataFrame( [low_TPR_scores_dict, low_FPR_scores_dict], index=['TPR_scores', 'FPR_scores'] ).T\n",
    "    low_df.to_csv( score_save_path / 'low_id_ood_scores.csv', index=True)\n",
    "\n",
    "    # calculate AURCO\n",
    "    low_score_auroc = metrics.auc(list(low_FPR_scores_dict.values()), list(low_TPR_scores_dict.values()))\n",
    "    \n",
    "    # save scores AURCO\n",
    "    with open(score_save_path / 'low_auroc_score.txt', 'w') as f:\n",
    "        f.write(str(low_score_auroc))\n",
    "\n",
    "    # Calculate FPR95\n",
    "    low_tpr_list = pd.Series(low_TPR_scores_dict.values())\n",
    "    low_fpr_list = pd.Series(low_FPR_scores_dict.values())\n",
    "\n",
    "    low_fpr95_interpolate = calculate_fpr95(low_tpr_list, low_fpr_list)\n",
    "    low_tpr_closest, low_fpr95_closest = get_closest_fpr95(low_tpr_list, low_fpr_list)\n",
    "\n",
    "    # Save FPR95 scores\n",
    "    with open(score_save_path / 'low_fpr95_score.txt', 'w') as f:\n",
    "        f.write(f'FPR95 (interpolated): {low_fpr95_interpolate:.4f}\\n')\n",
    "        f.write(f'TPR closest to 0.95: {low_tpr_closest:.4f}\\n')\n",
    "        f.write(f'FPR95 (closest): {low_fpr95_closest:.4f}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8261dbd3",
   "metadata": {},
   "source": [
    "# Apply"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c4fd99",
   "metadata": {},
   "source": [
    "### Apply - over_all_species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "af65b368",
   "metadata": {},
   "outputs": [],
   "source": [
    "percentiles = np.arange(0.0,1.001,0.01)\n",
    "percentiles = np.round(percentiles, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b1064c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_predict_scores_over_percentile(\n",
    "    prediction_dir_path / 'over_all_examples',\n",
    "    result_dir_path / 'over_all_examples',\n",
    "    percentiles\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c7d373",
   "metadata": {},
   "source": [
    "# knn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77376cb5",
   "metadata": {},
   "source": [
    "### Apply - for_each_species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9e04b9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_predict_scores_over_percentile(\n",
    "    prediction_dir_path / 'knn/for_each_species',\n",
    "    result_dir_path / 'knn/for_each_species',\n",
    "    percentiles\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74941fc5",
   "metadata": {},
   "source": [
    "### Apply - in each species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a44cfb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_predict_scores_over_percentile(\n",
    "    prediction_dir_path / 'knn/in_each_species',\n",
    "    result_dir_path / 'knn/in_each_species',\n",
    "    percentiles\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b7ab8d",
   "metadata": {},
   "source": [
    "# mlp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6f4ea9",
   "metadata": {},
   "source": [
    "### Apply - for_each_species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "21ce5070",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_predict_scores_over_percentile(\n",
    "    prediction_dir_path / 'mlp/for_each_species',\n",
    "    result_dir_path / 'mlp/for_each_species',\n",
    "    percentiles\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70afd112",
   "metadata": {},
   "source": [
    "### Apply - in each species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "73a2c736",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_predict_scores_over_percentile(\n",
    "    prediction_dir_path / 'mlp/in_each_species',\n",
    "    result_dir_path / 'mlp/in_each_species',\n",
    "    percentiles\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masterArbeit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
